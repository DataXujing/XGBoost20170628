<!DOCTYPE html>
<html>
  <head>
    <title>XGBoost神一样的存在着</title>
    <meta charset="utf-8">
    <meta name="author" content="徐静 @ 联信全栈数据工程师培训" />
    <link rel="stylesheet" href="zh-CN.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# XGBoost神一样的存在着
### 徐静 @ 联信全栈数据工程师培训
### 2017/6/27

---




class: inverse,middle
background-image: url(pic/star.jpg)




1. 为什么是**XGBoost** &lt;sup&gt;[1]&lt;/sup&gt;?
--

2. **XGBoost** 与 GBDT 理论
--

3. 数据科学中用R/Python实现**XGBoost**
--

4. **XGBoost**如何应用到催收业务?
--


.footnote[[1]不做特殊说明，下文中的**XGBoost**简写为**xgb**]


---
class: inverse,center, middle
background-image: url(pic/star.jpg)

# 1.为什么是XGBoost

---
class: inverse,  middle

background-image: url(pic/star.jpg)

.pull-left[

**xgb**在企业中的应用

+ [**xgb**的分布式版本被用于阿里云大数据计算服务ODPS](https://yq.aliyun.com/articles/6355) 

+ 使用**xgb**的Graphlab Create

+ 腾讯数据平台把分布式版本的**xgb**用于微信内的购买点击预测

+ 汽车之家把分布式版本的**xgb**用于展示广告的点击率预测
]

.pull-right[

**xgb**占据各大数据科学比赛

+ 横扫Kaggle大赛的**xgb**（去年的29个获奖方案中，有17个是用**xgb**）

+ [使用**xgb**的Kaggle获奖方案可在这里找到](https://www.kaggle.com/kernels)

+ [使用**xgb**的Seldon预测服务Iris](http://docs.seldon.io/iris-demo.html)

+ GitHub上的项目：[dmlc/xgboost](https://github.com/dmlc/xgboost/tree/master/demo#usecases)

]



---
class: inverse,  middle,center

background-image: url(pic/star.jpg)

xgb与深度学习到底孰优孰劣？都说xgb好用，为什么名气总不如深度学习？ 

---
class: inverse

background-image: url(pic/star.jpg)

问题1：AlphaGo大战柯洁、李世石后，所有人都能谈上几句深度学习。人工智能在围棋上的这场突破，最终还要归功于机器学习三巨头三十年如一日的长期研究。相比之下，:**xgb**名气可就小太多了。更何况，它的发起人还只是个"名不见经传"(开玩笑啊)的年轻人(华盛顿大学--陈天奇),针对非常要求准确度的那些问题，**xgb**确实很有优势，同时它的计算特性也很不错。然而，相对于支持向量机、随机森林或深度学习，**xgb**的优势倒也没到那种夸张的程度。特别是当你拥有足够的训练数据，并能找到合适的深度神经网络时，深度学习的效果就明显能好上一大截。
还有用户打趣说，**xgb**的名气坏在它的名字上，深度学习一听就非常高大上，**xgb**再怎么包装也是书呆子气十足。
那么，**xgb**的发起人又是怎么说的呢？

--

**xgb**的发起人——陈天奇博士，他并不认可将深度学习和**xgb**截然对立起来。他谈到，这两种方法在其各自擅长领域的性能表现都非常好：

+	**xgb**专注于模型的可解释性，而基于人工神经网络的深度学习，则更关注模型的准确度。

+	**xgb**更适用于变量数较少的表格数据，而深度学习则更适用于图像或其他拥有海量变量的数据。

---

class: inverse,middle

background-image: url(pic/star.jpg)

问题2：**xgb**与深度学习孰优孰劣？

--

不同的机器学习模型适用于不同类型的任务。深度神经网络通过对时空位置建模，能够很好地捕获图像、语音、文本等高维数据。而基于树模型的**xgb**则能很好地处理表格数据，同时还拥有一些深度神经网络所没有的特性（如：模型的可解释性、输入数据的不变性、更易于调参等）。

---
class: inverse,middle,center

background-image: url(pic/star.jpg)


**xgb**与深度学习并不冲突，是调参比赛大杀器！


---

class: inverse,middle,center

background-image: url(pic/star.jpg)

# 2. XGBoost与GBDT理论

---
class: inverse,middle

background-image: url(pic/star.jpg)

+ GBDT（又称Gradient Boosted Decision Tree/Grdient Boosted Regression Tree），是一种迭代的决策树算法，该算法由多个决策树组成。它最早见于yahoo，后被广泛应用在搜索排序、点击率预估上。

--

+  GBDT是一个基于迭代累加的决策树算法，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。
树模型也分为决策树和回归树，决策树常用来分类问题，回归树常用来预测问题。决策树常用于分类标签值，比如用户性别、网页是否是垃圾页面、用户是不是作弊；而回归树常用于预测真实数值，比如用户的年龄、用户点击的概率、网页相关程度等等。由于GBDT的核心在与累加所有树的结果作为最终结果，而分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），所以**GBDT中的树都是回归树**（其实回归树也能用来做分类）。

--

+ 优点：不需要特征的归一化，损失函数多样，自动特征选择，模型可解释性好。

--

+ 缺点：boosting是个串行过程不能并行计算，复杂度高，不适合高维稀疏特征。

---


class: inverse,middle
background-image: url(pic/star.jpg)

### LogLoss情形下的GBDT推导



目标是寻找使得期望损失最小的决策函数，，我们要求其具有一定的形式：即是一组弱学习器的加性和。

`$$F^{\ast}=argminE_{x,y}[L(y,F(x))]$$`
`$$F(x;\rho_m,\alpha_m)=\sum_{m=0}^{M}\rho_m h(x;\alpha_m)$$`
这里的 `\(\rho\)` 是步长， `\(h\)` 是第 `\(m\)` 棵树的预测值，使用梯度下降法求解最优参数

---

class: inverse,middle
background-image: url(pic/star.jpg)

算法：

`\(F^{\ast}=argminE_{x,y}[L(y,F(x))|x]\)`

`\(F_0(x)=f_0(x)\)`


for `\(m=1,2,...M\)`:  # `\(M\)` 为分类器的个数（树的个数）

`$$g_m(x)=-\frac{\partial E_y[L(y,F(x)|x]}{\partial F(x)}|_{F(x)=F_{m-1}(x)}$$`

`$$\rho_m=argminE_y[L(y,F_{m-1}(x)+\rho g_m(x)|x]$$`
`$$f_m(x)=\rho_m g_m(x)$$`
`$$F_m(x)=F_{m-1}(x)+\rho_mg_m(x)$$`
end for 

`\(F_{\ast}\approx F_M(x) = f_0(x)+\sum_{m=1}^M\rho_mg_m(x)\)`


---

class: inverse,middle
background-image: url(pic/star.jpg)

需要估计 `\(F_0(x)=f_0(x)\)` , `\(g_m(x)\)` , `\(\rho_m\)` ,可以使用决策树逼近，使用LogLoss损失函数进行推导：

`$$L(y,F)=log(1+exp(-2yF))，y \in \{-1,1\}$$`

---

class: inverse,middle
background-image: url(pic/star.jpg)

第一步：求解 `\(F_0(x)\)`

`$$F_0(x)=argmin\sum_{i=1}^{N}L(y_i,F)$$`
`$$\frac{\partial \sum_{i=1}^N L(y_i,F)}{\partial F}=0$$`

`$$F_0(x)=\frac{1}{2}log\frac{1+\bar{y}}{1-\bar{y}}$$`

---

class: inverse,middle
background-image: url(pic/star.jpg)

第二步：估计 `\(g_m(x)\)` (梯度)，实现时是第m颗树需要拟合的残差,并用决策树对其拟合

`$$g_m(x_i)=- \frac{\partial L(y_i,F)}{\partial F}|_{F=F_{m-1}}$$`

`$$g_m(x_i)=2y_i/(1+exp\{2y_iF_{m-1}(x_i)\})$$`

---

class: inverse,middle
background-image: url(pic/star.jpg)

第三步：牛顿法求解下降方向步长 `\(\rho_m\)`

`$$f(r)=\sum_{x_i \in R_{jm}} log(1+exp(-2y_i(F_{m-1}(x_i)+r)))$$`
`$$f^{'}(r)=\sum_{x_i \in R_{jm}}\frac{-2y_i}{1+exp(2y_i(F_{m-1}(x_i)+r))}$$`
`$$f^{''}(r)=\sum_{x_i \in R_{jm}} \frac{2y_iexp(2y_i(F_{m-1}(x_i))+r)}{[1+exp(2y_i(F_{m-1}(x_i)+r))]^2}$$`
`$$r_{jm}=r_0-f^{'}(r_0)/f^{''}(r_0)$$`

---


class: inverse,middle
background-image: url(pic/star.jpg)

通常实现时第三步被省略，通过Shrinkage的策略设置步长

--
+ Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。用方程来看更清晰，即
没用Shrinkage时：( `\(y_i\)` 表示第 `\(i\)` 棵树上 `\(y\)` 的预测值， `\(y_{(1 \sim i)}\)` 表示前 `\(i\)` 棵树 `\(y\)` 的综合预测值)。
**没有Shrinkage**时 `\(y_{(1 \sim i)}=\sum_{l=1}^{i}y_l\)` ，**加Shrinkage**时： `\(y_{(1 \sim i)}=\sum_{l=1}^{i-1}y_l+ step.y_i\)`
 
--
 
+ 即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001，导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。这个weight就是step。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。

---


class: inverse,middle
background-image: url(pic/star.jpg)

第四步：预测，把每棵树的预测值乘以缩放因子加到一起就得到预测值啦

`$$predict=predict(0)+\sum_{m=1}^{M}Shrinkage.predict(d_m)$$`
如果需要输出到 `\((0,1)\)`之间，则：

`$$probability=\frac{1}{1+exp(-2predict)}$$`


---
class: inverse,middle
background-image: url(pic/star.jpg)

### GBDT常用参数和模型调优

1. 树个数

2. 树深度

3. 缩放因子

4. 损失函数

5. 数据采样比

6. 特征采样比

---
class: inverse,middle
background-image: url(pic/star.jpg)

更多的学习资源：

+ http://blog.csdn.net/china1000/article/details/51106856

+ http://blog.csdn.net/w28971023/article/details/8240756

+ http://blog.csdn.net/puqutogether/article/details/41957089

+ https://wenku.baidu.com/view/7b3b625a28ea81c759f5784a.html

+ http://www.cnblogs.com/pinard/p/6140514.html

+ http://www.0791quanquan.com/news_keji/topic_2083422/

+ http://scikit-learn.org/stable/


---

class: inverse,middle
background-image: url(pic/star.jpg)

### XGBoost的理论推导&lt;sup&gt;[1]&lt;/sup&gt;

+ e&lt;span style="color:red";&gt;X&lt;/span&gt;treme &lt;span style="color:red";&gt;G&lt;/span&gt;radient &lt;span style="color:red";&gt;B&lt;/span&gt;oosting，Gradient Boosting算法的一种升级版

+ 传统GBDT只是利用了一阶导数信息（第三步用到了二阶导数信息，但第三步往往省略）

+ XGBoost对损失函数做了二阶泰勒展开

+ XGBoost在目标函数之外加入了正则项整体求最优解



.footnote[[1] 下述推导来自陈天奇关于XGBoost的PPT]


---

class: inverse,middle
background-image: url(pic/star.jpg)

### 1. 实现方式的选择

+ Objective(目标函数)： `\(\sum_{i=1}^{n}l(y_i,\hat{y_i})+\sum_{k}\Omega(f_k)， f_k \in \mathcal{F}\)`

+ 并没有使用SGD寻找 `\(f\)`

+ 方案：Additive Training (Boosting)

`$$\hat{y}_i^{(0)}=0$$`
`$$\hat{y}_i^{(1)}=f_1(x_i)=\hat{y}_i^{(0)}+f_1(x_i)$$`
`$$\hat{y}_i^{(2)}=f_1(x_i)+f_2(x_i)=\hat{y}_i^{(1)}+f_2(x_i)$$`
`$$......$$`


---

class: inverse,middle
background-image: url(pic/star.jpg)

###2. Additive Training

+ 如何决定增加哪一个 `\(f\)` ?

+ 第 `\(t\)` 轮的预测为： `\(\hat{y}_i^{(t)}=\hat{y}_i^{(t-1)}+f_t(x_i)\)` ,其中 `\(f_t(x_i)\)` 是我们在第 `\(t\)` 轮需要确定的。

+ `\(Obj^{(t)}=\sum_{i=1}^{n}l(y_i,\hat{y}_i^{(t)})+\sum_{i=1}^{t}\Omega(f_i)\)`

+ `\(Obj^{(t)}=\sum_{i=1}^{n}l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)+constant\)` 找到 `\(f_t\)` 使之最小化。

+ 这里可以用平方损失函数（就是我们常说的残差）。


---

class: inverse,middle
background-image: url(pic/star.jpg)

### 3. Taylor Expansion Approximation of Loss

+ 目标： `\(Obj^{(t)}=\sum_{i=1}^{n}l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i))+\Omega(f_t)+constant\)`

--
+ Taylor展开: `\(f(x+\Delta x)=f(x)+f^{'}(x)\Delta x+\frac{1}{2}f^{''}(x)\Delta x^2\)`

--
+ 令： `\(g_i=\partial _{\hat{y}^{(t-1)}}l(y_i,\hat{y}^{(t-1)})\)` , `\(h_i=\partial _{\hat{y}^{(t-1)}}^2l(y_i,\hat{y}^{(t-1)})\)`

--
+ `\(Obj^{(t)}=\sum_{i=1}^{n}[l(y_i,\hat{y}_i^{(t-1)})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)+constant\)`

--
+ 常数项可以被拿掉

+ 实在不明白就拿残差展开看一下


---


### 4. 改善一下弱分类器的定义

![](pic/1.png)

---


### 5. 定义弱分类器的复杂度

![](pic/2.png)



---
class: inverse,middle
background-image: url(pic/star.jpg)

### 6. 带入目标函数

+ 所有在叶子 `\(j\)` 的实例： `\(I_j=\{i|q(x_i)=j\}\)`

+ 目标函数可重新定义为：
`$$Obj^{(t)}=\sum_{j=1}^T[(\sum_{i \in I_j}g_i)\omega_j+\frac{1}{2}(\sum_{i \in I_j}h_i+\lambda)\omega_j^2]+\gamma^T$$`

+ 形式上可以看出这是 `\(T\)` 个独立的二次函数之和


---
class: inverse,middle
background-image: url(pic/star.jpg)

### 7. 继续处理

+ 定义： `\(G_j=\sum_{i \in I_j}g_i\)` , `\(H_j=\sum_{i \in I_j}h_i\)`

+ `\(Obj^{(t)}=\sum_{j=1}^{T}[G_jw_j+\frac{1}{2}(H_j+\lambda)\omega_j^2]+\gamma^T\)`

+ 通过对 `\(\omega_j\)` 求导等于0，可以得到 `\(\omega_j^{\ast}=-\frac{G_j}{H_j+\lambda}\)`

+ 然后把 `\(\omega_j\)` 最优解带入得到 `\(Obj=-\frac{1}{2}\sum_{t=1}^{T}\frac{G_j^2}{H_j+\lambda}+\gamma^T\)`


---
class: inverse,middle
background-image: url(pic/star.jpg)

+ &lt;p style="color:yellow";&gt;最终实现最小化损失函数的方向上选择最优的加性分类器&lt;/p&gt;

+ &lt;p style="color:yellow";&gt;最小化的过程中GBDT用了在梯度方向上的递减，而XGBoost展开到二阶导&lt;/p&gt;




---

class: inverse,middle
background-image: url(pic/star.jpg)

### 8. XGBoost中的参数(1)


General Parameters(通用参数)：
+ booster：所使用的模型，gbtree或gblinear
+ silent：1则不打印提示信息，0则打印，默认为0
+ nthread：所使用的线程数量，默认为最大可用数量
Booster Parameters（gbtree）(辅助参数)：
+ eta：学习率，默认初始化为0.3，经多轮迭代后一般衰减到0.01至0.2
+ min_child_weight：每个子节点所需的最小权重和，默认为1
+ max_depth：树的最大深度，默认为6，一般为3至10
+ max_leaf_nodes：叶结点最大数量，默认为 `\(2^6\)`
+ gamma：拆分节点时所需的最小损失衰减，默认为0
+ max_delta_step：默认为0


---
class: inverse,middle
background-image: url(pic/star.jpg)

### 8. XGBoost中的参数(2)

+ subsample：每棵树采样的样本数量比例，默认为1，一般取0.5至1
+ colsample_bytree：每棵树采样的特征数量比例，默认为1，一般取0.5至1
+ colsample_bylevel：默认为1
+ lambda：L2正则化项，默认为1
+ alpha：L1正则化项，默认为1
+ scale_pos_weight：加快收敛速度，默认为1
Learning Task Parameters(任务参数)：
+ objective：目标函数，默认为reg:linear，还可取binary:logistic、multi:softmax、multi:softprob
+ eval_metric：误差函数，回归默认为rmse，分类默认为error，其他可取值包括rmse、mae、logloss、merror、mlogloss、auc
+ seed：随机数种子，默认为0

---

class: inverse,middle
background-image: url(pic/star.jpg)

**xgb**更多参考资料：

+ http://blog.csdn.net/a819825294/article/details/51206410

+ http://www.cnblogs.com/mfryf/p/6238185.html

+ https://github.com/DataXujing/fullstack-data-engineer

+ https://xgboost.readthedocs.io/en/latest/build.html

+ https://github.com/Honlan/fullstack-data-engineer/tree/master/data/Parameter_Tuning_XGBoost_with_Example

+ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-XGBoost-with-codes-python/

+ 感兴趣可以深入研究分布式XGBoost的设计理念

---

class: inverse,middle,center
background-image: url(pic/star.jpg)

#R/Python实现XGBoost

---

class: inverse,middle
background-image: url(pic/star.jpg)

+ R 代码：打开[XGBoost.R](demo/XGBoost.R) (交叉验证，调参等不用R了太慢了)

+ Python代码：打开[XGBoost_Py文件夹](demo/XGBoost_py)(启用Jupyter)

+ 真正的数据科学绝决方案可能要复杂的多，需要多模型的融合，以[Rong360比赛](demo/青岛黄渤团队)为例

+ 后续我们会逐渐弃用R中的shiny,Rmarkdown等工具，选择Flask开发一些嵌入机器学习的在线web应用

---

class: inverse, middle
background-image: url(pic/star.jpg)

+ 我们的催收业务中有太多的预测，分类问题（比如回收率的预测，回款的预测，案件的分类，客户的分类，债务人的分类，员工考核的分类......)

+ 我们不能坐以待毙等待IT的二期工程

+ Flask，数据库，机器学习，深度学习，在联信服务器上部署自己的web应用

+ 现在还在研究阶段欢迎加入。


---
class: inverse, middle, center
background-image: url(pic/star.jpg)

# 蟹蟹

资源已上传至[GitHub](https://github.com/DataXujing?tab=repositories)
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
